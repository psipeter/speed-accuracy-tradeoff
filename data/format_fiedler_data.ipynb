{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cae232-e0fd-4f22-994d-361ad650fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e3039-d52e-4ca6-8729-cdd24eee88f6",
   "metadata": {},
   "source": [
    "## Convert .xlsx file into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b9ba55-f372-40c0-8f42-e65fac6cf0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding data for 11e92cd2764348faa18918c94947d4fa\n",
      "adding data for e00c31bc24424be5986b63504ef2572c\n",
      "adding data for 58c54d6d2775404a9c3a3cde65c32a71\n",
      "adding data for 159be9483bdc4dac871458482bbe7c64\n",
      "adding data for 54bbae95c6954858b3c4546072c7c7a6\n",
      "adding data for a1d579a793c1449b944b66bd66e498c9\n",
      "adding data for 4c000592ba1641d4b00cda3032684321\n",
      "adding data for d0dd4b374ab345b5ad09786280bf9ecd\n",
      "adding data for d552b2c7fb97451d97de0c3d489d46a4\n",
      "adding data for 147db44cbc514f589f7b158c93cbe072\n",
      "adding data for 81289118898b4d4a8f65e96e38a10e43\n",
      "adding data for 3148f74b1ffb43cea29844334282c2bd\n",
      "adding data for c8ee025f8b60429bb194e1000f0297a1\n",
      "adding data for 47e5820c96ec48078f611d29178ecd3e\n",
      "adding data for fdcba947f0ec44d1b3e2ee0053c9449f\n",
      "adding data for 75bc02b1b30147bfbeef0623ef76c315\n",
      "adding data for 729d32346fde461a910c7887d7860f39\n",
      "adding data for 9ab9798ae227487385e959254bfbd1d8\n",
      "adding data for af39bdbe8af44ac4a48e709f78a6ac4d\n",
      "adding data for 54b5eb179e8d4105aebda1e14df103f4\n",
      "adding data for 7109ba1247e24f59b69a282910024804\n",
      "adding data for 2e6f20cf79b345c5bc20e9ed1e76a893\n",
      "adding data for 2d2da659ba8b4b719c0ec5f8945452c9\n",
      "adding data for 54e1614bf8564674b67ec59aa36fb451\n",
      "adding data for a074f426f2e14a3ebce2479be32f6498\n",
      "adding data for b9dcf88a0b7b48d08f60f5508d168e80\n",
      "adding data for ab5d43e7971e42399015eddd4135967b\n",
      "adding data for bdee2d88bbb04ff484477ba766b22ad0\n",
      "adding data for b1333405bc9a4e99bb44053afee65a3b\n",
      "adding data for d1cca6e22ee141c7bbdafb3e2cc2fdcc\n",
      "adding data for 7f6d8bad75f448da95241c86fe40aeb8\n",
      "adding data for c49874f64a5346c6afa3d847f48ea9e6\n",
      "adding data for d22df28e6ca741fdbb2bb6ec36ce8cbb\n",
      "adding data for 9c912c4b7c83407fb3d387d3aee7c098\n",
      "adding data for 50a48ab9a6524411baba1d48fc349a8a\n",
      "adding data for b2c0dd396eb24964a350c3eb7103be65\n",
      "adding data for 4481cde3e6a54d019084d97bd9998eee\n",
      "adding data for c1d5cdde06a04e59abbd183772db6181\n",
      "adding data for 3a2bd228962e44e4b55114c4091d11be\n",
      "adding data for a04788fb29a04f22b99f15dbe0e4b4d3\n",
      "adding data for b31473a97ca849eab506474bf624c25c\n",
      "adding data for 9df3d884bb47476298f5b0a7fe6a86f9\n",
      "adding data for 848a9280a9904cb3b18571f2fc514bcf\n",
      "adding data for c96017be239848a4b394386913f13026\n",
      "adding data for ece1f226b161426aafd433aa0e933b5d\n",
      "adding data for de41155c10694acaa03cb9e97bb7e0c9\n",
      "adding data for d73737b5e0524c0bb07f350cadf4e51e\n",
      "adding data for ad3b086e423b45bb82e378d1ba2f2668\n",
      "adding data for b92ff07d0d19447db1b722c51ad3bf92\n",
      "adding data for eb7b16b6fd0e47dab6a50960a530b28d\n",
      "adding data for 8bb7ce21913c4ef991dce9fc657d9f15\n",
      "adding data for af22d19e7e5c40fa8eca9913e96e0590\n",
      "adding data for 883945169dbd472d800c96da5a5a990f\n",
      "adding data for 1dd4868b80034a789234bbbf9cbc05f1\n",
      "adding data for 7a7341e9499241eab30f89ae65337aaa\n",
      "adding data for ec550d60b04e41a28a2ee1754a8ee7b0\n",
      "adding data for e73de1f8cbef48d19e3bd1e1561137bc\n",
      "adding data for ff3e3b96dc694b9cbd6f7c028ad2b905\n",
      "adding data for 0e5017c8623b4c1fa8d3c209a8cf4953\n",
      "adding data for c91a32f5be744a398357d18cedf26bf6\n",
      "adding data for d69175aaa78049ffa8f89787975d4ebb\n",
      "adding data for 12d8da1d3ca84c3899edc82bad99fc23\n",
      "adding data for a347f4a790a24f8fbcc5a41b458329cf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter Duggins\\AppData\\Local\\Temp\\ipykernel_3820\\2029128291.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fiedler1 = pd.concat(dfs1, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "raw = pd.read_excel(\"fiedler_raw.xlsx\", header=0, dtype='object')\n",
    "\n",
    "columns1 = ['type', 'pid', 'id', 'difficulty', 'trial', 'position', 'cue', 'value', 'fraction_sampled', 'sampled_cues', 'max_cues', \n",
    "            'chosen', 'target', 'accuracy', 'cue_choice_aligned']\n",
    "columns2 = ['type', 'pid', 'id', 'dP', 'trial', 'accuracy', 'cues', 'max_cues']\n",
    "dfs1 = []\n",
    "dfs2 = []\n",
    "\n",
    "sid = 0\n",
    "for pid in raw['participant_id'].unique():\n",
    "    print(f'adding data for {pid}')\n",
    "    d1 = raw.query(\"participant_id==@pid\")\n",
    "    # print(d1)\n",
    "    for trial, timestamp in enumerate(d1['timestamp'].unique()):\n",
    "        d2 = d1.query(\"timestamp==@timestamp\")\n",
    "        # print(d2)\n",
    "        sampled_cues = d2['ticks'].to_numpy()[0]\n",
    "        max_cues = d2['sample_size'].to_numpy()[0]\n",
    "        started = 'R' if d2['started_right'].to_numpy()[0] == 1 else 'L'\n",
    "        Ls = [int(d) for d in str(d2['left'].to_numpy()[0])]\n",
    "        Rs = [int(d) for d in str(d2['right'].to_numpy()[0])]\n",
    "        delta = float(d2['delta'].to_numpy()[0])\n",
    "        if delta==0.4: difficulty = 'easy'\n",
    "        if delta==0.2: difficulty = 'moderate'\n",
    "        if delta==0.1: difficulty = 'hard'\n",
    "        chosen = 'R' if d2['selected_right'].to_numpy()[0]==1 else 'L'\n",
    "        target = 'R' if d2['p2'].to_numpy()[0] > d2['p1'].to_numpy()[0] else 'L'\n",
    "        accuracy = 1.0 if chosen==target else 0.0\n",
    "        # print('Ls', Ls)\n",
    "        # print('Rs', Rs) \n",
    "        for p in range(sampled_cues):\n",
    "            if started=='R':\n",
    "                cue = 'R' if (p % 2 == 0) else 'L'\n",
    "            elif started=='L':\n",
    "                cue = 'L' if (p % 2 == 0) else 'R'\n",
    "            if cue == 'R':\n",
    "                value = Rs[int(p/2)]\n",
    "            elif cue == 'L':\n",
    "                value = Ls[int(p/2)]\n",
    "            fraction_sampled = (p+1) / sampled_cues\n",
    "            cue_choice_aligned = 0.0\n",
    "            if chosen=='R':\n",
    "                if (cue=='R' and value==1) or (cue=='L' and value==0):\n",
    "                    cue_choice_aligned += 1\n",
    "                else:\n",
    "                    cue_choice_aligned += -1\n",
    "            elif chosen=='L':\n",
    "                if (cue=='L' and value==1) or (cue=='R' and value==0):\n",
    "                    cue_choice_aligned += 1\n",
    "                else:\n",
    "                    cue_choice_aligned += -1\n",
    "            # print(p, cue, value, fraction_sampled)\n",
    "            df = pd.DataFrame([[\n",
    "                'human', pid, sid, difficulty, trial, p, cue, value, fraction_sampled, sampled_cues, max_cues,\n",
    "                chosen, target, accuracy, cue_choice_aligned\n",
    "                ]], columns=columns1)\n",
    "            dfs1.append(df)\n",
    "        if sampled_cues==0:  # participant chose before sampling any cues\n",
    "            df = pd.DataFrame([[\n",
    "                'human', pid, sid, difficulty, trial, -1, None, None, None, 0, max_cues, chosen, target, accuracy, None\n",
    "                ]], columns=columns1)\n",
    "            dfs1.append(df)\n",
    "        df = pd.DataFrame([[\n",
    "                'human', pid, sid, delta, trial, accuracy, sampled_cues, max_cues\n",
    "                ]], columns=columns2)\n",
    "        dfs2.append(df)\n",
    "    sid += 1\n",
    "\n",
    "fiedler1 = pd.concat(dfs1, ignore_index=True)\n",
    "fiedler2 = pd.concat(dfs2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38bab8-6cd2-465e-abe6-e7b3af589a29",
   "metadata": {},
   "source": [
    "## Drop participants that don't meet the inclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78035f98-0b10-4cea-bbff-097db20faff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting 11e92cd2764348faa18918c94947d4fa, who only completed 1 trials\n",
      "cutting d552b2c7fb97451d97de0c3d489d46a4, who failed to sample any cues on 902 trials\n",
      "cutting c8ee025f8b60429bb194e1000f0297a1, who only completed 18 trials\n",
      "cutting 75bc02b1b30147bfbeef0623ef76c315, who only completed 11 trials\n",
      "cutting 729d32346fde461a910c7887d7860f39, who only completed 6 trials\n",
      "cutting 9ab9798ae227487385e959254bfbd1d8, who failed to sample any cues on 834 trials\n",
      "cutting af39bdbe8af44ac4a48e709f78a6ac4d, who only completed 2 trials\n",
      "cutting 54b5eb179e8d4105aebda1e14df103f4, who only completed 21 trials\n"
     ]
    }
   ],
   "source": [
    "exclude = []\n",
    "for pid in fiedler2['pid'].unique():\n",
    "    # cut participants who completed less than 30 trials (excludes 5 people)\n",
    "    if len(fiedler2.query('pid==@pid')['trial'].unique()) < 30:  \n",
    "        if pid not in exclude:\n",
    "            print(f\"cutting {pid}, who only completed {len(fiedler2.query('pid==@pid')['trial'].unique())} trials\")\n",
    "            exclude.append(pid)\n",
    "    # cut participants who chose before sampling evidence 30 or more times (excluces 2 people)\n",
    "    if len(fiedler2.query('pid==@pid & cues==0')['trial'].unique()) > 30:  \n",
    "        if pid not in exclude:\n",
    "            print(f\"cutting {pid}, who failed to sample any cues on {len(fiedler2.query('pid==@pid & cues==0')['trial'].unique())} trials\")\n",
    "            exclude.append(pid)\n",
    "\n",
    "for pid in exclude:\n",
    "    fiedler1 = fiedler1.drop(fiedler1[fiedler1.pid==pid].index)\n",
    "    fiedler2 = fiedler2.drop(fiedler2[fiedler2.pid==pid].index)\n",
    "\n",
    "# cut all remaining data for participants who chose before sampling evidence\n",
    "fiedler1 = fiedler1.drop(fiedler1[fiedler1.sampled_cues==0].index)\n",
    "\n",
    "fiedler1.to_pickle(\"fiedler_position.pkl\")  # each row contains data on one sample from one trial\n",
    "fiedler2.to_pickle(\"fiedler_trial.pkl\")  # each row contains collapsed data from one trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b65a03-c790-4504-8270-64f24626130e",
   "metadata": {},
   "source": [
    "## bin and average ```fraction_sampled``` and ```cue_choice_aligned``` data from ```fiedler1.pkl```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946be817-4203-4ad6-b56a-2d30a1f74bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0.0, 1.2, 0.2)\n",
    "columns = ['type', 'pid', 'id', 'difficulty', 'max_cues', 'fraction_sampled', 'mean_cue_choice_aligned', 'std_cue_choice_aligned']\n",
    "dfs = []\n",
    "for pid in fiedler1['pid'].unique():\n",
    "    sid = fiedler1.query('pid==@pid')['id'].unique()[0]\n",
    "    for difficulty in fiedler1['difficulty'].unique():\n",
    "        for max_cues in fiedler1['max_cues'].unique():\n",
    "            data = fiedler1.query('pid==@pid & difficulty==@difficulty & max_cues==@max_cues')\n",
    "            for i in range(len(bins)-1):\n",
    "                left = bins[i]\n",
    "                right = bins[i+1]\n",
    "                midpoint = (left + right) / 2\n",
    "                cue_choices_aligned = data.query('fraction_sampled>@left & fraction_sampled<=@right')['cue_choice_aligned'].to_numpy()\n",
    "                mean = np.mean(cue_choices_aligned)\n",
    "                std = np.std(cue_choices_aligned)\n",
    "                df = pd.DataFrame([[\n",
    "                    'human', pid, sid, difficulty, max_cues, midpoint, mean, std,\n",
    "                    ]], columns=columns)\n",
    "                dfs.append(df)\n",
    "fiedler3 = pd.concat(dfs, ignore_index=True)\n",
    "fiedler3.to_pickle(\"fiedler_binned.pkl\")  # each row contains average and std data from one participant in one condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366bf82-7d90-462a-b0a9-2a440b567f08",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eaf5d8-56f9-42a6-8b99-fc4d7d551dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_extraction()\n",
    "    data = pd.read_excel(\"fiedler_raw.xlsx\", header=0)\n",
    "    # remove test data, then remove redundant \"tag\" field\n",
    "    for i, tag in enumerate(data['tag'].unique()):\n",
    "        if type(tag)=='float':\n",
    "            continue  # data wasn't label with tester tag\n",
    "    #         if type(tag)=='float' and np.isnan(tag):\n",
    "    #             data = data.drop(data[data.tag==tag].index)  # suspicious lack of tag\n",
    "        if tag in [\"Testversion\", \"TEST JOHANNES / LOL\", \"999\", \"Johannes Test 2\"]:\n",
    "            data = data.drop(data[data.tag==tag].index)\n",
    "        if tag in [\"Testversion\", \"TEST JOHANNES / LOL\", \"999\", \"Johannes Test 2\"]:\n",
    "            data = data.drop(data[data.tag==tag].index)\n",
    "    data = data.drop(columns=['tag'])\n",
    "    # rename sample_size to maxSamples\n",
    "    data = data.rename(columns={'sample_size': 'max_cues'})\n",
    "    # rename ticks to cues, left to A, and right to B, p1 to pA, p2 to pB\n",
    "    data = data.rename(columns={'ticks': 'cues', 'left': 'A', 'right': 'B', 'p1': 'pA', 'p2': 'pB'})\n",
    "    # add a \"correct\" column based on \"selected right\" and comparing \"p1\" to \"p2\"\n",
    "    chosen_answer = data['selected_right'].to_numpy()\n",
    "    correct_answer = data['pB'].to_numpy() > data['pA'].to_numpy()\n",
    "    correct = chosen_answer == correct_answer\n",
    "    data['accuracy'] = 1.0*correct\n",
    "    data = data.drop(columns=[\n",
    "        'timestamp', 'subrange_key', 'duration_ms', 'empirical_delta', 'empirical_p1', 'empirical_p2',\n",
    "        'selected_right', 'started_right', 'last_left', 'last_right',\n",
    "    ])\n",
    "    # remove bad participants (choose before 1st cue, or have insuficient trials)\n",
    "    data = data.drop(data[data.participant_id==\"11e92cd2764348faa18918c94947d4fa\"].index)\n",
    "    data = data.drop(data[data.participant_id==\"d552b2c7fb97451d97de0c3d489d46a4\"].index)\n",
    "    data = data.drop(data[data.participant_id==\"9ab9798ae227487385e959254bfbd1d8\"].index)\n",
    "    data = data.drop(data[data.participant_id==\"c49874f64a5346c6afa3d847f48ea9e6\"].index)\n",
    "    data.to_pickle(\"fiedler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009064c-7280-48f7-a06c-e81519b04504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_fiedler():\n",
    "    dfs = []\n",
    "    columns = ('type', 'id', 'dP', 'mean cues', 'var cues', 'mean acc', 'var acc')\n",
    "    emp = pd.read_pickle(\"data/fiedler2021.pkl\")\n",
    "    for pid in emp['id'].unique():\n",
    "        for dP in [0.4, 0.2, 0.1]:\n",
    "            subdata = emp.query(\"id==@pid & dP==@dP\")\n",
    "            mean_cues = subdata['cues'].mean()\n",
    "            var_cues = subdata['cues'].std()\n",
    "            mean_acc = subdata['accuracy'].mean()\n",
    "            var_acc = subdata['accuracy'].std()\n",
    "            dfs.append(pd.DataFrame([['human', pid, dP, mean_cues, var_cues, mean_acc, var_acc]], columns=columns))\n",
    "    new_emp = pd.concat(dfs, ignore_index=True)\n",
    "    new_emp.to_pickle(\"data/fiedler2021_condensed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85130e14-4074-4c4c-ace2-8656b4682445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_condensed():\n",
    "    new_emp = read_pickle(\"data/fiedler2021_condensed.pkl\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=((7,2)))\n",
    "    for i, dP in enumerate([0.4, 0.2, 0.1]):\n",
    "        subdata = new_emp.query(\"dP==@dP\")\n",
    "        x = subdata['mean cues'].to_numpy()\n",
    "        y = subdata['mean acc'].to_numpy()\n",
    "        xerr = subdata['var cues'].to_numpy()\n",
    "        yerr = subdata['var acc'].to_numpy()\n",
    "        axes[i].errorbar(x, y, xerr=xerr, yerr=0*yerr, fmt=\"o\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = sns.FacetGrid(new_emp, row=\"type\", col=\"dP\", col_order=[0.4, 0.2, 0.1], palette=palette, height=2, aspect=1)\n",
    "    fig.map_dataframe(sns.scatterplot, x=\"mean cues\", y=\"mean acc\")\n",
    "    fig.set_xlabels(\"Mean Cues\")\n",
    "    fig.set_ylabels(\"Mean Accuracy\")\n",
    "    fig.set(yticks=[50, 60, 70, 80, 90, 100])\n",
    "    fig.add_legend()\n",
    "    fig.savefig(f\"plots/facet2.svg\")\n",
    "    fig.savefig(f\"plots/facet2.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb317a68-e706-46d3-b441-f02622310212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remake_fiedler():\n",
    "    emp = pd.read_pickle(\"data/empirical.pkl\")\n",
    "    dfs = []\n",
    "    columns = ('type', 'id', 'dP', 'trial', 'accuracy', 'cues', 'max_cues')\n",
    "    for i, pid in enumerate(emp['participant_id'].unique()):\n",
    "        for dP in [0.4, 0.2, 0.1]:\n",
    "            # print(f\"pid {pid}, dP {dP}\")\n",
    "            subdata = emp.query(\"participant_id==@pid & delta==@dP\")\n",
    "            trial = 0\n",
    "            for index, row in subdata.iterrows():\n",
    "                dfs.append(pd.DataFrame([[\"human\", f\"{i}\", dP, trial, 100*row['correct'], row['cues'], row['maxSamples']]], columns=columns))\n",
    "                trial += 1\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.to_pickle(\"data/fiedler2021.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
